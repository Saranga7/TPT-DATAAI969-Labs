{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8yKkIzj11mB"
      },
      "source": [
        "### Small Intro to Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGVOoYlP16q7"
      },
      "source": [
        "Pytorch is very similar to Numpy since it also uses multidimensional Tensor objects with similar features (e.g., sDatatypes, Slicing, Broadcasting, Batch operations etc.). The most important differences between Numpy and Pytorch is the fact that Pytorch can use graphics processing units (GPU) to accelerate tensors operations and that it has a native optimized autograd engine for automatically computing derivatives (similar to `autograd.numpy`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i_5n_XuKr5k"
      },
      "source": [
        "If you are using Googel Colab, you can enable GPUs in \"Modifier\"/Paramètres du Notebook\"/\"Accélérateur Materiel\"\n",
        "\n",
        "\n",
        "Similarly to Numpy, PyTorch tensors have a `dtype` attribute specifying their datatype. All PyTorch tensors also have a `device` attribute that specifies the device where the tensor is stored -- either CPU, or CUDA (for NVIDA GPUs). A tensor on a CUDA device will automatically use the GPU to accelerate all operations.\n",
        "\n",
        "Just as with datatypes, we can use the [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) method to change the device of a tensor. We can also use the convenience methods `.cuda()` and `.cpu()` methods to move tensors between CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RkoFEVVKWlW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  print('PyTorch can use GPUs!')\n",
        "else:\n",
        "  print('PyTorch cannot use GPUs.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D03s614dMCvy"
      },
      "outputs": [],
      "source": [
        "# Construct a tensor on the CPU\n",
        "x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print('x0 device:', x0.device)\n",
        "\n",
        "# Move it to the GPU using .to()\n",
        "x1 = x0.to('cuda')\n",
        "print('x1 device:', x1.device)\n",
        "\n",
        "# Move it to the GPU using .cuda()\n",
        "x2 = x0.cuda()\n",
        "print('x2 device:', x2.device)\n",
        "\n",
        "# Move it back to the CPU using .to()\n",
        "x3 = x1.to('cpu')\n",
        "print('x3 device:', x3.device)\n",
        "\n",
        "# Move it back to the CPU using .cpu()\n",
        "x4 = x2.cpu()\n",
        "print('x4 device:', x4.device)\n",
        "\n",
        "# We can construct tensors directly on the GPU as well\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n",
        "print('y device / dtype:', y.device, y.dtype)\n",
        "\n",
        "# Calling x.to(y) where y is a tensor will return a copy of x with the same\n",
        "# device and dtype as y\n",
        "x5 = x0.to(y)\n",
        "print('x5 device / dtype:', x5.device, x5.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TDxICdOmJo"
      },
      "source": [
        "Performing large tensor operations on a GPU can be **a lot faster** than running the equivalent operation on CPU.\n",
        "\n",
        "Here we compare the speed of adding two tensors of shape (10000, 10000) on CPU and GPU:\n",
        "\n",
        "(Note that GPU code may run asynchronously with CPU code, so when timing the speed of operations on the GPU it is important to use `torch.cuda.synchronize` to synchronize the CPU and GPU.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW14ZF-_PK7t"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "a_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "b_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "\n",
        "a_gpu = a_cpu.cuda()\n",
        "b_gpu = b_cpu.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "t0 = time.time()\n",
        "c_cpu = a_cpu + b_cpu\n",
        "t1 = time.time()\n",
        "c_gpu = a_gpu + b_gpu\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "# Check that they computed the same thing\n",
        "diff = (c_gpu.cpu() - c_cpu).abs().max().item()\n",
        "print('Max difference between c_gpu and c_cpu:', diff)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del a_cpu, b_cpu, a_gpu, b_gpu"
      ],
      "metadata": {
        "id": "ovnAsrGglbAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEAVPEwviYb"
      },
      "source": [
        "You should see that running the same computation on the GPU was more than 10-30 times faster than on the CPU! (run it seeral times, if it's not the case) Due to the massive speedups that GPUs offer, we will use GPUs to accelerate much of our machine learning code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vYEWEwUx81w"
      },
      "source": [
        "A list of functions for vector/matrix product can be found [`here`](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations). Some examples are:\n",
        "\n",
        "- [`torch.dot`](https://pytorch.org/docs/stable/torch.html#torch.dot): Computes inner product of vectors\n",
        "- [`torch.mm`](https://pytorch.org/docs/stable/torch.html#torch.mm): Computes matrix-matrix products\n",
        "- [`torch.mv`](https://pytorch.org/docs/stable/torch.html#torch.mv): Computes matrix-vector products\n",
        "- [`torch.addmm`](https://pytorch.org/docs/stable/torch.html#torch.addmm) / [`torch.addmv`](https://pytorch.org/docs/stable/torch.html#torch.addmv): Computes matrix-matrix and matrix-vector multiplications plus a bias\n",
        "- [`torch.bmm`](https://pytorch.org/docs/stable/torch.html#torch.addmv) / [`torch.baddmm`](https://pytorch.org/docs/stable/torch.html#torch.baddbmm): Batched versions of `torch.mm` and `torch.addmm`, respectively\n",
        "- [`torch.matmul`](https://pytorch.org/docs/stable/torch.html#torch.matmul): General matrix product that performs different operations depending on the rank of the inputs; this is similar to `np.dot` in numpy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHNfu2eZkucy"
      },
      "source": [
        "One of the most important operation in Deep Learning is the batched matrix multiplication 'bmm'. Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-VdCZB9x6Ni"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__)\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  print('PyTorch can use GPUs!')\n",
        "else:\n",
        "  print('PyTorch cannot use GPUs.')\n",
        "\n",
        "B, N, M, P = 3, 2, 5, 4\n",
        "x = torch.rand(B, N, M)  # Random tensor of shape (B, N, M)\n",
        "y = torch.rand(B, M, P)  # Random tensor of shape (B, M, P)\n",
        "\n",
        "# We can use a for loop to (inefficiently) compute a batch of matrix multiply\n",
        "# operations\n",
        "z1 = torch.empty(B, N, P)  # Empty tensor of shape (B, N, P)\n",
        "for i in range(B):\n",
        "  z1[i] = x[i].mm(y[i])\n",
        "print('Here is the result of batched matrix multiply with a loop:')\n",
        "print(z1)\n",
        "\n",
        "z2 = torch.bmm(x, y)\n",
        "print('\\nHere is the result of batched matrix multiply with bmm:')\n",
        "print(z2)\n",
        "\n",
        "diff = (z1 - z2).abs().max().item()\n",
        "print('\\nDifference:', diff)\n",
        "print('Difference within threshold:', diff < 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del x, y, z1, z2, diff"
      ],
      "metadata": {
        "id": "xbxKoSY6lvi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xStokygF05gh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "a_cpu = torch.randn(5000, 1000, 10, dtype=torch.float32)\n",
        "b_cpu = torch.randn(5000, 10, 100, dtype=torch.float32)\n",
        "\n",
        "a_gpu = a_cpu.cuda()\n",
        "b_gpu = b_cpu.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Compare batched version of torch in cpu and gpu\n",
        "t0 = time.time()\n",
        "c_cpu = torch.bmm(a_cpu, b_cpu)\n",
        "t1 = time.time()\n",
        "c_gpu = torch.bmm(a_gpu, b_gpu)\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "# Check that they computed the same thing\n",
        "#diff = (c_gpu - c_cpu).abs().max().item() # this will give an error\n",
        "diff = (c_gpu.cpu() - c_cpu).abs().max().item()\n",
        "print('Max difference between c_gpu and c_cpu:', diff)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VVm7OOAlYPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}