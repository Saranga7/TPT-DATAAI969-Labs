{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JdZ_NoJMUp1"
      },
      "source": [
        "## Contrastive pre-training\n",
        "\n",
        "Here, we will try the SimCLR method.\n",
        "\n",
        "[1] T. Chen et al. “A Simple Framework for Contrastive Learning of Visual Representations”. In: ICML. 2020.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LnJlAFB8jfv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "torch.manual_seed(42) # Setting the seed\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10, PCAM\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "## Plot Options\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "import seaborn as sns\n",
        "plt.set_cmap(\"cividis\")\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(666)\n",
        "torch.manual_seed(666)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbuBZiTN9o12"
      },
      "source": [
        "### Data Augmentation for Contrastive Learning\n",
        "\n",
        "One of the key points of SimCLR is the choice of the augmentation strategy. It composes many different geometric and iconographic transformations.\n",
        "We can implement them very efficiently and easily using the Dataset object of Pytorch.\n",
        "\n",
        "Since in SimCLR authors use 2 views, we do the same here. Please note that we could use more positives\n",
        "\n",
        "The transformations used are: (figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/simclr_data_augmentations.jpg\" width=\"800px\" style=\"padding-top: 10px; padding-bottom: 10px\"></center>\n",
        "\n",
        "When using ImageNet-derived datasets, the two most important transformations are: crop-and-resize, and color distortion.\n",
        "Interestingly, they need to be used together since, when combining randomly cropping and resizing, we might have two situations: (a) cropped image A provides a local view of cropped image B, or (b) cropped images C and D show neighboring views of the same image (figure credit - [Ting Chen and Geoffrey Hinton](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/crop_views.svg\" width=\"400px\" style=\"padding-top: 20px; padding-bottom: 0px\"></center>\n",
        "\n",
        "While situation (a) requires the model to learn some sort of scale invariance to make crops A and B similar in the representation space, situation (b) is more challenging since the model needs to recognize an object beyond its limited view.\n",
        "However, the network can use the color information (color histograms) to create a useless link between the two patches, without learning generalizable high-level representations. For instance, it could focus on the color of the fur of the dog and on the color of the background to understand that the two patches belong to the same image. That's why, we need to compose crop-and-resize and color distortion."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Create a Pytorch class of transformations (transforms.Compose) where you apply the following augmentations:\n",
        "- Resize an Crop\n",
        "- Horizontal Flip\n",
        "- Color jittering\n",
        "- Gray scale changes\n",
        "- Gaussian Blur\n",
        "\n",
        "Hint: all functions have already been implemented and can be found here: https://pytorch.org/vision/0.9/transforms.html\n",
        "\n",
        "Alternatively, you can also use [albumentations](https://albumentations.ai/) by simply adding:\n",
        "\n",
        "`import albumentations as A`\n",
        "\n",
        "`from albumentations.pytorch.transforms import ToTensorV2`"
      ],
      "metadata": {
        "id": "HcqluDet_9rZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE89L8tB9xAF"
      },
      "outputs": [],
      "source": [
        "class ContrastiveTransformations:\n",
        "    def __init__(self, img_size):\n",
        "      # transformations applied in SimCLR article\n",
        "      XXXXXXX\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # values from ImageNet, so that we can use algorithms pre-trained on ImageNet\n",
        "      XXXXXXX\n",
        "\n",
        "    def __call__(self, x):\n",
        "      # it outputs a tuple, namely 2 views (augmentations) fo the same image\n",
        "      return  XXXXXX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAd7xO_qA41"
      },
      "source": [
        "We create unlabeled, training and test Datasets.\n",
        "Please be careful since we use two different transformations, one for the unlabelled part and one for train/test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXhGEmnT9voP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "\n",
        "# simple transformation to use networks pre-trained on ImageNet\n",
        "img_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "unlabeled_dataset_2viewsCon = STL10(root=\"./data\", split=\"unlabeled\", download=True, transform=ContrastiveTransformations(96))\n",
        "train_dataset = STL10(root=\"./data\", split=\"train\", download=True, transform=img_transforms)\n",
        "# just to show the effect of the augmentations and the classes\n",
        "train_dataset_2viewsCon = STL10(root=\"./data\", split=\"train\", download=True, transform=ContrastiveTransformations(96))\n",
        "test_dataset = STL10(root=\"./data\", split=\"test\", download=True, transform=img_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: What's the difference between `ContrastiveTransformations` and `img_transforms` ? Look inside the functions and their outputs..."
      ],
      "metadata": {
        "id": "8dElKdfNCRyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMXiU1qX2dqu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# To check the classes in STL10\n",
        "classes=unlabeled_dataset_2viewsCon.classes\n",
        "print(classes)\n",
        "print('Number images in Unlabeled dataset:' ,len(unlabeled_dataset_2viewsCon))\n",
        "print(unlabeled_dataset_2viewsCon[0][0][0].shape) # this is one image (the first of the 2-views tuple)\n",
        "\n",
        "# Train dataset\n",
        "labels=train_dataset.labels # retrieve label of each sample\n",
        "print('Number images in Train dataset:' , len(train_dataset)) # retrieve length of dataset\n",
        "print(train_dataset[3][0].shape) # this is one image\n",
        "\n",
        "#Test dataset\n",
        "print('Number images in Test dataset:' ,len(test_dataset))\n",
        "print(test_dataset[0][0].shape) # this is one image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kioo1xFkQVRa"
      },
      "source": [
        "The Unlabeled dataset contains 100k images. Here, to limit memory requirement, we will use 10% of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbZTvz0RI6j_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "sizeUnlabelled=5000\n",
        "\n",
        "unlabeled_dataset_2viewsCon_red,rest = random_split(unlabeled_dataset_2viewsCon, [sizeUnlabelled, len(unlabeled_dataset_2viewsCon)-sizeUnlabelled])\n",
        "len(unlabeled_dataset_2viewsCon_red)\n",
        "del unlabeled_dataset_2viewsCon # free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create a function to visualize the views of each sample, based on the chosen augmentation strategy."
      ],
      "metadata": {
        "id": "sOtu79SO_tkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIH9otn52ZR1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def imshowSTL102views(datasetOrig,datasetTransform,rows=5,figsize=(8, 15)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    for i in range(1, 3*(rows-1)+2,3):\n",
        "      imgOrig = datasetOrig[i][0]\n",
        "      img1=datasetTransform[i][0][0]\n",
        "      img2=datasetTransform[i][0][1]\n",
        "\n",
        "      #REMOVE NORMALIZATION\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "      # Clip values to range [0,1] -> possible rounding errors during normalization\n",
        "      imgOrig = np.clip(unnormalize(imgOrig).numpy(),0,1)\n",
        "      img1 = np.clip(unnormalize(img1).numpy(),0,1)\n",
        "      img2 = np.clip(unnormalize(img2).numpy(),0,1)\n",
        "\n",
        "      label = datasetOrig[i][1]\n",
        "      fig.add_subplot(rows, 3, i)\n",
        "      plt.title(datasetOrig.classes[label]+ ' , original')\n",
        "      plt.imshow(np.transpose(imgOrig, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "      fig.add_subplot(rows, 3, i+1)\n",
        "      plt.title(datasetOrig.classes[label] + ' , 1st view')\n",
        "      plt.imshow(np.transpose(img1, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "      fig.add_subplot(rows, 3, i+2)\n",
        "      plt.title(datasetOrig.classes[label] + ' , 2nd view')\n",
        "      plt.imshow(np.transpose(img2, (1, 2, 0)))\n",
        "      plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KmQg476Jqtl"
      },
      "outputs": [],
      "source": [
        "imshowSTL102views(train_dataset,train_dataset_2viewsCon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGh01C9oQvuq"
      },
      "outputs": [],
      "source": [
        "del train_dataset_2viewsCon # To free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfghymIV9o17"
      },
      "source": [
        "Here, it's the most important part of the code.\n",
        "\n",
        "I remind you that the Siamese architecture of SimCLR is: (figure credit - [Ting Chen et al. ](https://arxiv.org/abs/2006.10029)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/13-contrastive-learning/simclr_network_setup.svg\" width=\"350px\"></center>\n",
        "\n",
        "The employed loss is the InfoNCE loss:\n",
        "$$\n",
        "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
        "$$\n",
        "where $\\tau$ is the temperature and the similarity measure is the cosine similarity:\n",
        "$$\n",
        "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
        "$$\n",
        "The maximum cosine similarity possible is $1$, while the minimum is $-1$.\n",
        "\n",
        "After training, we will remove the projection head $g(\\cdot)$, and use $f(\\cdot)$ as a pretrained feature extractor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Fill the code where you see **XXXXXXXXXXXXXXXXXX**"
      ],
      "metadata": {
        "id": "_wEaYlBvCYPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKD0fMKqrOv7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import shutil\n",
        "import yaml\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "class SimCLR():\n",
        "\n",
        "    def __init__(self, model, optimizer, scheduler, device, batch_size, temperature, epochs):\n",
        "        self.device=device\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.temperature=temperature\n",
        "        self.batch_size=batch_size\n",
        "        self.epochs=epochs\n",
        "\n",
        "\n",
        "    def info_nce_loss(self, features):\n",
        "\n",
        "        XXXXXXXX\n",
        "\n",
        "    def train(self, train_loader):\n",
        "\n",
        "        scaler = GradScaler() # gradient scaling, useful when we use float16\n",
        "\n",
        "        n_iter = 0\n",
        "        print(\"Start SimCLR training for {} epochs.\".format(self.epochs))\n",
        "\n",
        "        for epoch_counter in range(self.epochs):\n",
        "            for images, _ in tqdm(train_loader):\n",
        "\n",
        "                images = torch.cat(XXXXXXXXXX)\n",
        "\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                with autocast(dtype=torch.float16): # to improve performance while maintaining accuracy.\n",
        "                #with autocast():\n",
        "                    features = self.model(images)\n",
        "                    XXXXXXX\n",
        "                    loss = XXXXXXXXXXX\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                n_iter += 1\n",
        "\n",
        "            # warmup for the first 10 epochs\n",
        "            if epoch_counter >= 5:\n",
        "                self.scheduler.step()\n",
        "\n",
        "            print('Epoch: {}, Average loss: {:.4f}, lr: {:.4f}'.format(epoch_counter, loss / len(train_loader.dataset), self.scheduler.get_last_lr()[0] ))\n",
        "\n",
        "        print(\"Training has finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBzpMRY_9o18"
      },
      "source": [
        "As before, we use a DataLoader.\n",
        "\n",
        "DataLoader wraps an iterable around the Dataset to enable easy access to the samples. The Dataset retrieves our dataset features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python multiprocessing to speed up data retrieval. DataLoader is an iterable that abstracts this complexity for us in an easy API.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIfB8usLnw_H"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "bs = XXXXXXX # choose an appropriate batch size depending on the computational resources\n",
        "\n",
        "train_unlabelled_loader = DataLoader(dataset=unlabeled_dataset_2viewsCon_red, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZzHMeG09o19"
      },
      "source": [
        "Here we define the hyper-parameters, optimization, scheduler and launch the training.\n",
        "\n",
        "**Question**: complete the code where you see XXXXX.\n",
        "\n",
        "As model for $f()$, use a ResnNet18 not-pretrained. Remember that the model ResNet18 has already a Linear Layer at the end (fc) which can be written as $Wf(x)$. You can also change the `out_features` of the fc by adding the option `num_classes=XXXX` while loading the model.\n",
        "\n",
        " As projection head, use the one from the article\n",
        "\n",
        " $$ g(f(x))=W^a \\sigma (W^b f(x))$$\n",
        "\n",
        " where $\\sigma$ is a Relu non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iuDyoz2vmbL"
      },
      "outputs": [],
      "source": [
        "max_epochs=10\n",
        "lr=0.003\n",
        "wd=1e-4\n",
        "temperature=0.07\n",
        "#f_dim=512 # to use if you want to change the output dimension of f\n",
        "g_dim=128 # the output dimension of the projection head\n",
        "\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = XXXXXXXX\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_unlabelled_loader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, device=device, batch_size=bs, temperature=temperature, epochs=max_epochs)\n",
        "simclr.train(train_unlabelled_loader)\n",
        "\n",
        "# save model checkpoints\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_50epochs_stl10.pth.tar'\n",
        "torch.save({\n",
        "                'epoch': max_epochs,\n",
        "                'state_dict': simclr.model.state_dict()\n",
        "            }, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je6wLYpA9o19"
      },
      "source": [
        "To continue, you can use a model that I have already pre-trained for 50 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FER2FZWlIKJd"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import gdown\n",
        "\n",
        "# Creating dataset folder\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_100epochs_stl10.pth.tar'\n",
        "\n",
        "# Download the pre-trained model\n",
        "file_url = 'https://drive.google.com/uc?id=13_ZueA9mqh17GvYVkfU_Yokg3z065rKG'\n",
        "gdown.download(file_url, filename, quiet=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Since we will only use f, we can load the original resnet18 (ignoring the projector g)\n",
        "modelDownload = models.resnet18(weights=None)\n",
        "\n",
        "checkpoint = torch.load(filename, map_location=device)\n",
        "state_dict= checkpoint['state_dict']\n",
        "# strict=False is important since here we are only copying the values for f ignoring the projector g\n",
        "modelDownload.load_state_dict(state_dict, strict=False)\n",
        "epoch = checkpoint['epoch']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9itgta9o19"
      },
      "source": [
        "As previously explained, we can now use $f()$ to encode the samples discarding $g()$.\n",
        "\n",
        "\n",
        "**Question** How can you discard the projection ? Complete the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGMQg40HWGI8"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset, batchsize,  device):\n",
        "    # Prepare model\n",
        "    network = deepcopy(model)\n",
        "\n",
        "    # you need to modify network here\n",
        "    XXXXXXXXX\n",
        "\n",
        "    network.eval() # we are testing, parameters are frozen\n",
        "    network.to(device)\n",
        "\n",
        "    # Encode all images\n",
        "    data_loader = data.DataLoader(dataset, batch_size=batchsize, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "\n",
        "    feats, labels = [], []\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    # Sort images by labels\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "\n",
        "    return data.TensorDataset(feats, labels), [feats.numpy() , labels.numpy()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pc8-XK49o1-"
      },
      "source": [
        "We can use either the trained model or the donwladed model and encode the train and test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdYnp20mVBPu"
      },
      "outputs": [],
      "source": [
        "#modelTrained=simclr.model\n",
        "modelTrained=modelDownload\n",
        "trainloader, [train_feats, train_labels] = prepare_data_features(modelTrained, train_dataset, batchsize=256, device=device)\n",
        "testloader, [test_feats, test_labels] = prepare_data_features(modelTrained, test_dataset, batchsize=256, device=device)\n",
        "print(train_feats.shape, train_labels.shape)\n",
        "print(test_feats.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McW8aTD09o1-"
      },
      "source": [
        "**Linear Probe**:  we train a logistic regression on the train dataset and evaluate it on the test dataset. This is called Linear Probe.\n",
        "\n",
        "**Question**: compute the training and test errore using a logistic regression where you are free to use a regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5TR-EFodecU"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# compute linear probe results\n",
        "XXXXXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he8wW24A9o1-"
      },
      "source": [
        "What if we simply used a pre-trained model on ImageNet ? ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: use the previous function `prepare_data_features` with a resnet18 pre-trained on Imagenet (without simclr pretraining) and compute the Linear Probe as before."
      ],
      "metadata": {
        "id": "0vkM1zV3L9SF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jhl8MY89o1_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import torchvision.models as models\n",
        "\n",
        "modelImageNet = XXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P3-Uhri9o1_"
      },
      "source": [
        "**Question**: is the result better ? Why in your opinion ? What could you do to improve the method with the worst result ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HBhhuB09o1_"
      },
      "source": [
        "To go further, you could use the PatchCamelyon dataset (https://www.kaggle.com/datasets/andrewmvd/metastatic-tissue-classification-patchcamelyon).\n",
        "\n",
        "The PatchCamelyon benchmark is a image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue.\n",
        "\n",
        "You can donwload it from pytorch vision or, if you are using Google Colab, directly from our Google drive. The pytorch version needs to be unzipped and there is not enough RAM memory...\n",
        "\n",
        "Please do not change the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0779Lfnm9o1_"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# Creating dataset folder\n",
        "!mkdir /content/pcam\n",
        "\n",
        "# Download the Train set\n",
        "file_url = 'https://drive.google.com/uc?id=1ipIG12YWag54v2_2JIyfPiZDN0Eu3IjB'\n",
        "output_path = '/content/pcam/camelyonpatch_level_2_split_train_x.h5'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Download Valid set\n",
        "file_url = 'https://drive.google.com/uc?id=1emdhTV8J8Pv-SjKSoMzE_SbT04Ik2yUm'\n",
        "output_path = '/content/pcam/camelyonpatch_level_2_split_valid_x.h5'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Download Test set\n",
        "file_url = 'https://drive.google.com/uc?id=1dkeFapKSKm-wUtf9zicxiSHWIS0uxv8Z'\n",
        "output_path = '/content/pcam/camelyonpatch_level_2_split_test_x.h5'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Download the labels\n",
        "file_url = 'https://drive.google.com/uc?id=10ftBj2ZiiDESTsANdF-v8oh4NieinYPP'\n",
        "output_path = '/content/label.zip'\n",
        "gdown.download(file_url, output_path, quiet=False)\n",
        "\n",
        "# Unzip and move to data directory\n",
        "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_path[:-4])\n",
        "!mv /content/label/Labels/camelyonpatch_level_2_split_train_y.h5 /content/pcam\n",
        "!mv /content/label/Labels/camelyonpatch_level_2_split_test_y.h5 /content/pcam\n",
        "!mv /content/label/Labels/camelyonpatch_level_2_split_valid_y.h5 /content/pcam\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can load it using the PCAM Dataset and DataLoader (keep download as False since you have already downloaded the data)\n",
        "\n",
        "In the following, you will use the training split as if it was unlabeled, thus for the self-supervised part, then the val split as if it was the training set and the test split as test set."
      ],
      "metadata": {
        "id": "D-968M16HGUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size=96\n",
        "bs = 128\n",
        "\n",
        "# ### PCAM Dataset\n",
        "\n",
        "PCAM_unlabeled_dataset = PCAM(root=\"\", download=False, split='train', transform=ContrastiveTransformations(img_size))\n",
        "PCAM_train_dataset = PCAM(root=\"\", download=False, split='val', transform=transforms.ToTensor())\n",
        "PCAM_test_dataset = PCAM(root=\"\", download=False, split='test', transform=transforms.ToTensor())\n",
        "print('There are: ', len(PCAM_unlabeled_dataset), 'training images; ', len(PCAM_train_dataset), ' validation images; and ', len(PCAM_test_dataset), ' test images')\n",
        "\n",
        "\n",
        "PCAM_unlabeled_loader = DataLoader(dataset=PCAM_unlabeled_dataset, batch_size=bs, num_workers=NUM_WORKERS, shuffle=True)\n",
        "PCAM_train_loader = DataLoader(PCAM_train_dataset, batch_size=bs)\n",
        "PCAM_test_loader = DataLoader(PCAM_test_dataset, batch_size=bs)\n",
        "\n",
        "\n",
        "# Defining model and training options\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n"
      ],
      "metadata": {
        "id": "0oV0QM3pOjqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the images as before"
      ],
      "metadata": {
        "id": "5LC-5u1xHr3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "PCAM_images = torch.stack([PCAM_train_dataset[np.random.randint(len(PCAM_train_dataset))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(PCAM_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the PCAM dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "xiaxlpr-PfjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: As before, train the SimCLR algorithm on the unlabeled set and then train and test the Linear Probe.\n",
        "\n",
        "This time, you can initialize the model as a ResNet-18 pre-trained on ImageNet and then train the simclr model for just 10 epochs.\n",
        "If you want to speed up the computations (a bit) you can also reduce the size of the images"
      ],
      "metadata": {
        "id": "M3oak1EQJMAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "max_epochs=2\n",
        "lr=0.003\n",
        "wd=1e-4\n",
        "temperature=0.07\n",
        "# Ensure that you are using GPU and all CPU workers\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "model = models.resnet18(weights=None, num_classes=128)\n",
        "dim_mlp = model.fc.in_features\n",
        "model.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), model.fc)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_unlabelled_loader), eta_min=0, last_epoch=-1)\n",
        "\n",
        "simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler, device=device, batch_size=bs, temperature=temperature, epochs=max_epochs)\n",
        "simclr.train(PCAM_unlabeled_loader)\n",
        "\n",
        "# save model checkpoints\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "filename = 'models/resnet18_simclr_100epochs_PCAM.pth.tar'\n",
        "torch.save({\n",
        "                'epoch': max_epochs,\n",
        "                'state_dict': simclr.model.state_dict()\n",
        "            }, filename)"
      ],
      "metadata": {
        "id": "Jt6UjBRbQv-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Using the linear probe, test whether the self-supervised training (for 10 or more epochs) improves the results over a simple ResNet18 only pre-trained on ImageNet. What would you do to improve the results if you had more time ?"
      ],
      "metadata": {
        "id": "EtqzT3VKEx2N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "469_Y50HJG13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}