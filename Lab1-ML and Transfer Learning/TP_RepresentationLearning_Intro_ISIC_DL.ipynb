{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skin lesion classification"
      ],
      "metadata": {
        "id": "Ao3Aj_ZSzwQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deadline**: Upload this notebook (rename it as 'TP1-DL-YOUR-SURNAME.ipynb') to Ecampus/Moodle before the deadline.\n"
      ],
      "metadata": {
        "id": "TD2Mt3JRz1VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context**\n",
        "A skin lesion is defined as a superficial growth or patch of the skin that is visually different and/or has a different texture than its surrounding area. Skin lesions, such as moles or birthmarks, can degenerate and become melanoma, one of the deadliest skin cancer. Its incidence has been increasing during the last decades, especially in the areas mostly populated by white people.\n",
        "\n",
        "The most effective treatment is an early detection followed by surgical excision. This is why several approaches for melanoma detection have been proposed in the last years (non-invasive computer-aided diagnosis (CAD) ).\n",
        "\n",
        "**Data**\n",
        "You will have at your disposal the ISIC 2017 dataset (https://challenge.isic-archive.com/data/#2017) already pre-processed, resized and quality checked. It is divided into Training (N=2000), Validation (N=150) and Test (N=600) sets.\n",
        "\n",
        "**Goal**\n",
        "The goal of this practical session is to classify images of skin lesions as either benign (nevus or seborrheic_keratosis) or melanoma (binary classification) using machine and deep learning algorithms.\n",
        "\n",
        "In the first part of the TP, you will manually compute some features relevant to the skin lesion classification (feature engineering) and then classify images using \"classical\" ML algorithms such as, logistic regression, SVM and Random Forests.\n",
        "\n",
        "In the second part, you will test the features learnt with Deep Learning algorithms. You will first train from scratch well-known CNN architectures (VGG, ResNet, DenseNet, etc..) and then leverage the representations learnt by these networks on a pre-training from Imagenet (fine-tuning, full-restimation).\n",
        "\n",
        "Please complete the code where you see **\"XXXXXXXXX\"** and answer the **Questions**\n"
      ],
      "metadata": {
        "id": "12KVxsLdz83m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQkZ88OILkWb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.io import imread\n",
        "from skimage.io import imsave\n",
        "from skimage.transform import resize\n",
        "from skimage import color\n",
        "from skimage import measure\n",
        "from skimage import transform\n",
        "from skimage.color import rgb2gray\n",
        "from scipy import ndimage\n",
        "from scipy import stats\n",
        "from scipy.stats import gaussian_kde\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import AxesGrid\n",
        "import torch\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from PIL import ImageShow\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "from torch import optim,nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,Dataset, TensorDataset\n",
        "from torchvision import models,transforms\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "# torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  !pip install gdown==4.6.0 # with the following versions, there is an error\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can either download the data from my Google Drive or work locally."
      ],
      "metadata": {
        "id": "2vUnlFkU3SGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0r--DZ47OJ2"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  print(\"you are using google colab\")\n",
        "  import gdown\n",
        "  !mkdir ./data\n",
        "  gdown.download(id=\"1iH5hkRN0wCgGklUN5df9u2Ue3UXAR4xZ\", output='./data/TrainCropped.zip', quiet=False)\n",
        "  !unzip -qu \"./data/TrainCropped.zip\" -d \"./data\"\n",
        "  gdown.download(id=\"1lyRZuV9UST55AEqwSy4mqMmh5yHGI1FM\", output='./data/TestCropped.zip', quiet=False)\n",
        "  !unzip -qu \"./data/TestCropped.zip\" -d \"./data\"\n",
        "  gdown.download(id=\"1RLJOmqAnHCgiJ7qShQurpxNaRhjjPpJb\", output='./data/ValCropped.zip', quiet=False)\n",
        "  !unzip -qu \"./data/ValCropped.zip\" -d \"./data\"\n",
        "  !rm -rf ./data/TrainCropped.zip\n",
        "  !rm -rf ./data/TestCropped.zip\n",
        "  !rm -rf ./data/ValCropped.zip\n",
        "  path='./data/'\n",
        "else:\n",
        "  print('You are NOT using colab')\n",
        "  # we assume that folders of data are in the same folder as this jupyter notebook\n",
        "  path='' # if you change this path , you should also change idTRain, idVal and idTest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is an error (might happen with gdown) please upload the three files manually.\n",
        "Follow the following instructions:\n",
        "- go to the folder symbol on the left of your screen\n",
        "- click on the three vertical dots on the 'data' folder\n",
        "- upload (importer in french) the three folders\n",
        "That's it !"
      ],
      "metadata": {
        "id": "b2z2gHwk4tIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if IN_COLAB:\n",
        "#   !unzip -qu \"./data/TrainCropped.zip\" -d \"./data\"\n",
        "#   !unzip -qu \"./data/TestCropped.zip\" -d \"./data\"\n",
        "#   !unzip -qu \"./data/ValCropped.zip\" -d \"./data\"\n",
        "#   !rm -rf ./data/TrainCropped.zip\n",
        "#   !rm -rf ./data/TestCropped.zip\n",
        "#   !rm -rf ./data/ValCropped.zip\n",
        "#   path='./data/'"
      ],
      "metadata": {
        "id": "nFcmZwt_3c1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Deep Learning part, we strongly suggest using GPU"
      ],
      "metadata": {
        "id": "sZcvPTV43bYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1lKfubgpHdy"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('Is there a GPU card?', torch.cuda.is_available(),'\\nNumber of GPU cards: ', torch.cuda.device_count(), '\\nWhich card GPU?', torch.cuda.get_device_name(0))\n",
        "  print('Total GPU memory {1:.2f} GB. Free GPU memory {0:.2f} GB'.format(torch.cuda.mem_get_info()[0]/pow(10,9),torch.cuda.mem_get_info()[1]/pow(10,9)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data."
      ],
      "metadata": {
        "id": "UcVOKS5t3gnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YidL3vd9pNf3"
      },
      "outputs": [],
      "source": [
        "pathTrain=glob.glob(path + \"TrainCropped/*.jpg\")\n",
        "print(pathTrain)\n",
        "idTrain=np.copy(pathTrain)\n",
        "if IN_COLAB:\n",
        "    for i in np.arange(len(idTrain)): idTrain[i]=idTrain[i][20:-4]\n",
        "else:\n",
        "    for i in np.arange(len(idTrain)): idTrain[i]=idTrain[i][13:-4]\n",
        "#print(idTrain)\n",
        "print('There are', len(idTrain), 'Train images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKnL44cKqEl3"
      },
      "outputs": [],
      "source": [
        "pathVal=glob.glob(path + \"ValCropped/*.jpg\")\n",
        "idVal=np.copy(pathVal)\n",
        "if IN_COLAB:\n",
        "    for i in np.arange(len(idVal)): idVal[i]=idVal[i][18:-4]\n",
        "else:\n",
        "    for i in np.arange(len(idVal)): idVal[i]=idVal[i][11:-4]\n",
        "#print(idVal)\n",
        "print('There are', len(idVal) , 'Validation images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJLmMYRgpQFk"
      },
      "outputs": [],
      "source": [
        "pathTest=glob.glob(path + \"TestCropped/*.jpg\")\n",
        "idTest=np.copy(pathTest)\n",
        "if IN_COLAB:\n",
        "    for i in np.arange(len(idTest)): idTest[i]=idTest[i][19:-4]\n",
        "else:\n",
        "    for i in np.arange(len(idTest)): idTest[i]=idTest[i][12:-4]\n",
        "#print(idTest)\n",
        "print('There are', len(idTest) , 'Test images')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Metadata and Target values\n",
        "\n",
        "You have at your disposal also two metadata, the age and the sex. If you want, you can use them as features in the classification but be careful ! There are missing values\n",
        "\n",
        "We also load the target values (0 for benign and 1 for melanoma)"
      ],
      "metadata": {
        "id": "ngOYRqe9h-N1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3qeLjuB9F84"
      },
      "outputs": [],
      "source": [
        "Metatrain = pd.read_csv('./data/TrainCropped/ISIC-2017_Training_Data_metadata.csv')\n",
        "print(Metatrain.head(10))\n",
        "Groundtrain = pd.read_csv('./data/TrainCropped/ISIC-2017_Training_Part3_GroundTruth.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqB5ZLXeyo3q"
      },
      "outputs": [],
      "source": [
        "Ytrain=np.zeros(len(idTrain))\n",
        "for i in range(len(idTrain)):\n",
        "  name=idTrain[i]\n",
        "  index=Groundtrain[\"image_id\"].str.find(name)\n",
        "  max_index = index.argmax()\n",
        "  Ytrain[i]=int(Groundtrain[\"melanoma\"][max_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crTO9h5x5Fla"
      },
      "outputs": [],
      "source": [
        "Metaval = pd.read_csv('./data/ValCropped/ISIC-2017_Validation_Data_metadata.csv')\n",
        "Groundval = pd.read_csv('./data/ValCropped/ISIC-2017_Validation_Part3_GroundTruth.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7QFUNEtys_L"
      },
      "outputs": [],
      "source": [
        "Yval=np.zeros(len(idVal))\n",
        "for i in range(len(idVal)):\n",
        "  name=idVal[i]\n",
        "  index=Groundval[\"image_id\"].str.find(name)\n",
        "  max_index = index.argmax()\n",
        "  Yval[i]=int(Groundval[\"melanoma\"][max_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSzrYNS5dnH"
      },
      "outputs": [],
      "source": [
        "Metatest = pd.read_csv('./data/TestCropped/ISIC-2017_Test_v2_Data_metadata.csv')\n",
        "Groundtest = pd.read_csv('./data/TestCropped/ISIC-2017_Test_v2_Part3_GroundTruth.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZjgOfVqytE_"
      },
      "outputs": [],
      "source": [
        "Ytest=np.zeros(len(idTest))\n",
        "for i in range(len(idTest)):\n",
        "  name=idTest[i]\n",
        "  index=Groundtest[\"image_id\"].str.find(name)\n",
        "  max_index = index.argmax()\n",
        "  Ytest[i]=int(Groundtest[\"melanoma\"][max_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSx_jQAXxMvG"
      },
      "source": [
        "##Deep Learning\n",
        "In this section, you will try simple Deep Learning strategies:\n",
        "- Fine-tuning the last layer of a network already trained on Image-Net\n",
        "- Re-training completely a network already trained on Image-Net\n",
        "- Re-training from scratch a network that has already shown good performances on other data-sets (architecture transfer or inductive bias transfer)\n",
        "\n",
        "You will have to use at least three different networks (e.g., ResNet, VGG, DenseNet)\n",
        "\n",
        "**Please compute the full time (reading papers/tutorials, coding, computational time) you spend on this part. It will be asked at the end of the practical session**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch also offers two nice primitives for storing and working with datasets:\n",
        "\n",
        "*   **Dataset** stores the images, labels and segmentation masks\n",
        "*   **DataLoader** wraps an iterable around the elements of the Dataset\n",
        "\n",
        "This is very practical since we can easily resize the images to the same size (important for DL) and multiply image and segmentation masks. Here, you have an exemple."
      ],
      "metadata": {
        "id": "9heR0Je9QA0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ISICDataset(Dataset):\n",
        "    def __init__(self, pathlist, targets, size=(224,224)):\n",
        "        self.image_paths = pathlist\n",
        "        self.mask_paths = [ p[:-4]+'seg.png' for p in pathlist ]\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.size=size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        y = self.targets[idx]\n",
        "\n",
        "        x = Image.open(img_path).resize(self.size, Image.BILINEAR) # all images are resized to (224,224)\n",
        "        s = Image.open(mask_path).resize(self.size, Image.NEAREST) # all segmentation masks are resized to (224,224)\n",
        "\n",
        "        # Multiply image and segmentation mask\n",
        "        blank = x.point(lambda _: 0)\n",
        "        c = Image.composite(x, blank, s)\n",
        "\n",
        "        # Send to tensor\n",
        "        x = TF.to_tensor(x)\n",
        "        c = TF.to_tensor(c)\n",
        "\n",
        "        # Normalize according to ImageNet statistics\n",
        "        x = TF.normalize(x,(0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        c = TF.normalize(c,(0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "        return x, c, y"
      ],
      "metadata": {
        "id": "xHTjTKxD0zhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Another reason why DataSets and DataLoaders are practical is that we can automatically apply data augmentation strategies. For instance, if we want to automatically apply (the same) data augmentations to both images and segmentations, we can use:"
      ],
      "metadata": {
        "id": "XJouPwpy06Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "class ISICDatasetWithAug(Dataset):\n",
        "    def __init__(self, pathlist, targets, size=(224,224)):\n",
        "        self.image_paths = pathlist\n",
        "        self.mask_paths = [ p[:-4]+'seg.png' for p in pathlist ]\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.size=size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        y = self.targets[idx]\n",
        "\n",
        "        x = Image.open(img_path).resize(self.size, Image.BILINEAR) # all images are resized to (224,224)\n",
        "        s = Image.open(mask_path).resize(self.size, Image.NEAREST) # all segmentation masks are resized to (224,224)\n",
        "\n",
        "        # Random crop\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(x, output_size=(128, 128))\n",
        "        x = TF.crop(x, i, j, h, w)\n",
        "        s = TF.crop(s, i, j, h, w)\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if np.random.random() > 0.5:\n",
        "          x = TF.hflip(x)\n",
        "          s = TF.hflip(s)\n",
        "\n",
        "        # Random vertical flipping\n",
        "        if np.random.random() > 0.5:\n",
        "            x = TF.vflip(x)\n",
        "            s = TF.vflip(s)\n",
        "\n",
        "        # Random rotation\n",
        "        angle=np.random.randint(-90, 90)\n",
        "        x=TF.rotate(x, angle, InterpolationMode.BILINEAR)\n",
        "        s=TF.rotate(s,angle, InterpolationMode.NEAREST)\n",
        "\n",
        "        # Multiply image and segmentation mask\n",
        "        blank = x.point(lambda _: 0)\n",
        "        c = Image.composite(x, blank, s)\n",
        "\n",
        "        # Send to tensor\n",
        "        x = TF.to_tensor(x)\n",
        "        c = TF.to_tensor(c)\n",
        "\n",
        "        # Normalize according to ImageNet statistics\n",
        "        x = TF.normalize(x,(0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        c = TF.normalize(c,(0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "        return x, c, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8TNzXlS12PBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have probably noticed that there are three outputs.\n",
        "\n",
        "**Question**: Explain what they are and which data augmentations we are computing."
      ],
      "metadata": {
        "id": "2y54RPw_1QD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XXXXXXX"
      ],
      "metadata": {
        "id": "JpDie1C4LwCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can load the data"
      ],
      "metadata": {
        "id": "y1uTyUxHRggO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0jifSbi2hqF"
      },
      "outputs": [],
      "source": [
        "# to make the results reproducible\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "torch.cuda.manual_seed(10)\n",
        "\n",
        "# Ensure that you are using GPU\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "batch_size=256 # adapted to the Google Colab GPU\n",
        "num_epochs=1 # to be modified\n",
        "learning_rate=0.05 # to be modified\n",
        "\n",
        "train_dataset = ISICDataset(pathTrain, Ytrain)\n",
        "val_dataset=ISICDataset(pathVal, Yval)\n",
        "test_dataset=ISICDataset(pathTest, Ytest)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, num_workers=1, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, num_workers=1, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, num_workers=1, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmgna2Enf2C_"
      },
      "source": [
        "The first part will be about fine-tuning the last layer of a network already pre-trained on Image-Net. Here it is the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Recx5E7gGxL"
      },
      "outputs": [],
      "source": [
        "num_classes = 1 # since we use the BCEWithLogitsLoss, we only have one output (if you were using CrossEntropy Loss, num_classes would be 2)\n",
        "\n",
        "# Load a pre-trained ResNet-18 model\n",
        "resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
        "# Freeze all parameters of the model\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "# Change last layer, the fully connected layer (classifier)\n",
        "resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can train this last layer on the training data. Find the best model in the validation set and evaluate the generalization performance on the test set. If you look for the best hyper-parameter, you should also use the validation set."
      ],
      "metadata": {
        "id": "DP3g_gR8153U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model=resnet18.float().cuda() # use float32 to save a bit of memory\n",
        "\n",
        "# pos_weight is N_Train/sum(Ytrain) -> why in your opinion ?\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([5.347593582887701]).cuda())\n",
        "auc = torchmetrics.AUROC(task='binary',num_classes=2, average = 'macro').cuda()\n",
        "accuracy = torchmetrics.Accuracy(task='binary',num_classes = 2, average='micro').cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "Tensor = torch.cuda.FloatTensor # use float32 to save a bit of memory\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for images, composite, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        #outputs = model(images.float().cuda())\n",
        "        outputs = model(composite.float().cuda())\n",
        "        loss = criterion(outputs.squeeze(), labels.float().cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #train_loss += loss.item() * images.size(0)\n",
        "        train_loss += loss.item() * composite.size(0)\n",
        "    train_loss = train_loss / len(train_dataset)\n",
        "\n",
        "    all_preds=[]\n",
        "    all_labels=[]\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, composite, labels in tqdm(val_loader):\n",
        "            #outputs = model(images.float().cuda())\n",
        "            outputs = model(composite.float().cuda())\n",
        "            loss = criterion(outputs.squeeze(), labels.float().cuda())\n",
        "            #val_loss += loss.item() * images.size(0)\n",
        "            val_loss += loss.item() * composite.size(0)\n",
        "            preds=torch.sigmoid(outputs).squeeze()\n",
        "            auc.update(preds, labels.cuda())\n",
        "            accuracy.update(preds, labels.cuda())\n",
        "    val_loss = val_loss / len(val_dataset)\n",
        "    val_auc = auc.compute()\n",
        "    val_acc = accuracy.compute()\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val AUC: {:.4f}, Val Acc: {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss, val_auc, val_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "Sq8f0opZIN6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing\n",
        "model.eval()\n",
        "test_loss=0\n",
        "with torch.no_grad():\n",
        "    for images, composite, labels in tqdm(test_loader):\n",
        "        outputs = model(images.float().cuda())\n",
        "        loss = criterion(outputs.squeeze(), labels.float().cuda())\n",
        "        test_loss += loss.item()\n",
        "        preds=torch.sigmoid(outputs).squeeze()\n",
        "        auc.update(preds, labels.cuda())\n",
        "        accuracy.update(preds, labels.cuda())\n",
        "test_loss = test_loss / len(test_dataset)\n",
        "test_auc = auc.compute()\n",
        "test_acc = accuracy.compute()\n",
        "print('\\nTest Loss: {:.4f}, Test AUC: {:.4f}, Test Acc: {:.4f}'.format(test_loss, test_auc, test_acc))"
      ],
      "metadata": {
        "id": "TjqyXZdP2Wiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00v0U86f30J"
      },
      "source": [
        "Now, it's time to retrain the entire network (previously pre-trained on Imagenet)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XXXXXXXXXX"
      ],
      "metadata": {
        "id": "VOS6CDtpQkF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8zmoX3EgFQ7"
      },
      "source": [
        "And eventually train from scratch the network, namely the weights should be randomly initiliazed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "XXXXXXXXXX"
      ],
      "metadata": {
        "id": "WzdVIm_WQkh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Train at least 3 different networks using the three strategies. Use the validation set to compare the performance of the three models and evaluate the best-performing one on the test set. Which is the best strategy ? Are you satisfied ?"
      ],
      "metadata": {
        "id": "ATUgL0Ia2wxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XXXXXXXX"
      ],
      "metadata": {
        "id": "Z5NTyyAILrQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: You can use as input to your model either the full image or the masked image. Compare the two strategies by testing *in both cases* only on the full images (assume that you do not have the segmentation masks at test time). In this way, the comparison will be more fair. What's better ?"
      ],
      "metadata": {
        "id": "8M6hCw-03FJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XXXXXX"
      ],
      "metadata": {
        "id": "a7GQz7vpLsy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Plot on a graph the time you have spent (x-axis) and the maximum AUC (y-axis) for all strategies you tried: feature engineering and DL fie-tuning or training. What's your conclusion ?"
      ],
      "metadata": {
        "id": "eoAxK-J34BCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XXXXXXX"
      ],
      "metadata": {
        "id": "-dLfQcnVLtnY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "560eu9yW3_BF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}